{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import scipy.sparse as sps\n",
    "import re\n",
    "from scipy import sparse\n",
    "from nltk import bigrams    \n",
    "from unicodedata import normalize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "leitura = pd.read_csv('date/estadao_noticias_eleicao.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_clear(text):\n",
    "    pattern = re.compile('[^a-zA-Z0-9 ]')\n",
    "    text = normalize('NFKD', text).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    return pattern.sub(' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [text_clear(stopword) for stopword in stopwords.words('portuguese')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "conteudos = leitura.titulo + \" \" + leitura.subTitulo +  \" \" + leitura.conteudo\n",
    "conteudos  = conteudos.fillna(\"\")\n",
    "conteudos = conteudos.apply(text_clear)\n",
    "ids = leitura.idNoticia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "noticias = conteudos.apply(nltk.word_tokenize)\n",
    "freq_term = noticias.apply(Counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_lists = conteudos.apply(lambda text: text.lower().split())\n",
    "noticias_token = [token for tokens_list in tokens_lists for token in tokens_list if token not in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = {}\n",
    "for i in range(len(noticias)):\n",
    "    id_noticia = ids[i]\n",
    "    for palavra in noticias[i]:\n",
    "        palavra = palavra.lower()\n",
    "        if palavra not in index:\n",
    "            index[palavra] = {}\n",
    "        id_rec = index[palavra].get(id_noticia)\n",
    "        \n",
    "        if not id_rec:\n",
    "            docs = index[palavra]\n",
    "            docs[id_noticia] = freq_term[i][palavra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_docs_peso(termos):\n",
    "    docs_peso = {}\n",
    "    \n",
    "    for i in range(len(termos)):\n",
    "        termo = termos[i]\n",
    "        docs = index[termo]\n",
    "        for doc_id in docs:\n",
    "            tf = docs[doc_id]\n",
    "            \n",
    "            if doc_id not in docs_peso:\n",
    "                docs_peso[doc_id] = np.array([0 if j != i else tf for j in range(len(termos))])\n",
    "            else:\n",
    "                doc_vector = docs_peso[doc_id]\n",
    "                doc_vector[i] = tf\n",
    "    return docs_peso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_query(phase):\n",
    "    query = np.array([1 if index.get(termo) else 0 for termo in phase])\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seach_tf(phase):\n",
    "    docs_tf = generator_docs_peso(phase)\n",
    "    query = generator_query(phase)\n",
    "    doc_rank = sorted(list(docs_tf.items()), key=lambda doc: np.dot(doc[1], query), reverse=True)[:5] \n",
    "    return [doc[0] for doc in doc_rank]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def co_occurrence_matrix(corpus):\n",
    "    vocab = set(corpus)\n",
    "    vocab = list(vocab)\n",
    "    n = len(vocab)\n",
    "   \n",
    "    vocab_to_index = {word:i for i, word in enumerate(vocab)}\n",
    "    \n",
    "    bi_grams = list(bigrams(corpus))\n",
    "\n",
    "    bigram_freq = nltk.FreqDist(bi_grams).most_common(len(bi_grams))\n",
    "\n",
    "    I=list()\n",
    "    J=list()\n",
    "    V=list()\n",
    "    \n",
    "    for bigram in bigram_freq:\n",
    "        current = bigram[0][1]\n",
    "        previous = bigram[0][0]\n",
    "        count = bigram[1]\n",
    "\n",
    "        I.append(vocab_to_index[previous])\n",
    "        J.append(vocab_to_index[current])\n",
    "        V.append(count)\n",
    "        \n",
    "    co_occurrence_matrix = sparse.coo_matrix((V,(I,J)), shape=(n,n))\n",
    "\n",
    "    return co_occurrence_matrix, vocab_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix, vocab =co_occurrence_matrix(noticias_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consult Frequencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_consult = matrix.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consult_frequency(w1, w2):\n",
    "    return(consultable_matrix[vocab[w1],vocab[w2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ocorrence(palavra):\n",
    "    list_occurency = matrix_consult[vocab[palavra]].getrow(0).toarray()[0]\n",
    "    indexs, frequency = zip(*sorted(enumerate(list_occurency), key=lambda x: x[1], reverse=True))\n",
    "    return indexs[:3], frequency[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2871, 2890, 3954, 4235, 4750]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2871, 2890, 3954, 4235, 4750]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "termo = 'lula'\n",
    "ocurrecy = get_ocorrence(termo)\n",
    "expansao = [word for key in ocurrecy[0] for word in vocab.keys() if vocab[word] == key]\n",
    "expansao.append(termo)\n",
    "seach_tf(expansao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
